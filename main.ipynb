{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmailWriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json, re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 950 ms, total: 11.1 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m = json.load(open('LBS_pattern.json', 'r'))\n",
    "model = defaultdict(lambda: defaultdict(Counter))\n",
    "for k, els in m.items():\n",
    "    for p, els2 in els.items():\n",
    "        for ng, count in els2.items():\n",
    "            model[k][tuple(p.split())][ng] = int(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "#doc = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(cat:str, sentence:str, cmp_wd:int = 2, printable=True):\n",
    "    top5 = []\n",
    "    categ = model[cat]\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [tok.lemma_ if '-' not in tok.lemma_ else 'SOMEONE' for tok in doc]\n",
    "    #print(tokens)\n",
    "    \n",
    "    # 比對 sentence 和 ngram pattern是否一樣\n",
    "    for p, els in categ.items():\n",
    "        # 先比較 last 2 gram\n",
    "        if len(tokens) >= 2 and tokens[-cmp_wd:] == list(p[:cmp_wd]):\n",
    "            top5.append([sum(els.values()), ' '.join(p), list(els.items()), cmp_wd])\n",
    "        # 再比較 last 1 gram\n",
    "        elif tokens[-cmp_wd:] != list(p[:cmp_wd]) and tokens[-(cmp_wd-1):] == list(p[:cmp_wd-1]):\n",
    "            top5.append([sum(els.values()), ' '.join(p), list(els.items()), cmp_wd-1])\n",
    "        # 再比較 last 2-nd gram\n",
    "        elif len(tokens) >= 2 and tokens[-cmp_wd:] != list(p[:cmp_wd]) and tokens[-(cmp_wd-1):] != list(p[:cmp_wd-1]) and tokens[-cmp_wd] == p[cmp_wd-2]:\n",
    "            top5.append([sum(els.values()), ' '.join(p), list(els.items()), cmp_wd-1])\n",
    "    \n",
    "    #print(top5)\n",
    "    # find top 5 pattern\n",
    "    cat_pat = sorted(top5, key=lambda x: (-(x[3]*x[0])))\n",
    "    # sort elements in each pattern by their counts\n",
    "    for idx in range(len(cat_pat)):\n",
    "        cat_pat[idx][2] = sorted(cat_pat[idx][2], key = lambda x: (-x[1], -(len(x[0].split()))))\n",
    "    \n",
    "    if not printable: return cat_pat\n",
    "    \n",
    "    print('category [%s]'%(cat.upper()))\n",
    "    for total, pat, ng, _ in cat_pat[:5]:\n",
    "        print()\n",
    "        print('%s\\t[%d]'%(pat, total))\n",
    "        for idx in range(len(ng)):\n",
    "            if idx == 3: break\n",
    "            print('\\t    [%s] %s\\t[%d]'%(re.findall('\\([A-Z]+\\)', ng[idx][0])[0][1:-1], re.sub('\\([A-Z]+\\)', '', ng[idx][0]), ng[idx][1]))\n",
    "        if len(ng) >=3 and idx == 3:\n",
    "            print('\\t         ...')\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category [SUGGESTION]\n",
      "\n",
      "take out n.\t[4]\n",
      "\t    [VERB] take out the home equity loan\t[2]\n",
      "\t    [VERB] take out a loan\t[1]\n",
      "\t    [VERB] take out bankruptcy\t[1]\n",
      "\n",
      "\n",
      "take n. to\t[3]\n",
      "\t    [VERB] take a little time to\t[1]\n",
      "\t    [VERB] take a moment to\t[1]\n",
      "\t    [VERB] take care to\t[1]\n",
      "\n",
      "\n",
      "take SOMEONE to n.\t[3]\n",
      "\t    [VERB] take you to lunch\t[2]\n",
      "\t    [VERB] take them to a grocery store\t[1]\n",
      "\n",
      "\n",
      "take a n. to\t[2]\n",
      "\t    [VERB] take a moment to\t[1]\n",
      "\t    [VERB] take a minute to\t[1]\n",
      "\n",
      "\n",
      "take n. to v.\t[2]\n",
      "\t    [VERB] take a moment to read\t[1]\n",
      "\t    [VERB] take care to include\t[1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = find_pattern('suggestion', 'Traveling to have a business meeting takes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category [ACCEPTANCE]\n",
      "\n",
      "and SOMEONE v.\t[31]\n",
      "\t    [CCONJ] and I will\t[16]\n",
      "\t    [CCONJ] and I look\t[5]\n",
      "\t    [CCONJ] and I plan\t[3]\n",
      "\t         ...\n",
      "\n",
      "\n",
      "and I v.\t[27]\n",
      "\t    [CCONJ] and I will\t[16]\n",
      "\t    [CCONJ] and I look\t[5]\n",
      "\t    [CCONJ] and I plan\t[3]\n",
      "\t         ...\n",
      "\n",
      "\n",
      "and v. n.\t[15]\n",
      "\t    [CCONJ] and expect a long and rewarding association\t[1]\n",
      "\t    [CCONJ] and organizing this company 's marketing branch\t[1]\n",
      "\t    [CCONJ] and select a mutually acceptable time\t[1]\n",
      "\t         ...\n",
      "\n",
      "\n",
      "and SOMEONE v. be\t[10]\n",
      "\t    [CCONJ] and I will be\t[10]\n",
      "\n",
      "\n",
      "and I v. be\t[10]\n",
      "\t    [CCONJ] and I will be\t[10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = find_pattern('acceptance', 'Mr. and fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = json.load(open('eval.json','r'))\n",
    "align = open('alignment.txt', 'r').read().strip().split('\\n')\n",
    "align = dict([a.split() for a in align])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [15:34<00:00, 18.69s/it]\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "hit = False\n",
    "for k, v in tqdm(evals.items()):\n",
    "    if align[k] != 'X':\n",
    "        c = align[k]\n",
    "        score.append([])\n",
    "        for document in v:\n",
    "            doc = nlp(document)\n",
    "            tok, pos = list(zip(*[(tok.text, tok.pos_) for tok in doc]))\n",
    "            sub_score, sub_total = 0, 0\n",
    "            for idx in range(2, len(tok)-1):\n",
    "                if pos[idx-1] == 'PUNCT' or pos[idx+1] == 'PUNCT':\n",
    "                    continue\n",
    "                else:\n",
    "                    sug = find_pattern(c, ' '.join(tok[:idx]), printable=False)\n",
    "                    if len(sug) == 0: continue\n",
    "                    sub_total += 1\n",
    "                    ans, ans2 = ' '.join(tok[idx-1:idx+1]), ' '.join(tok[idx-2:idx+1])\n",
    "                    for num, (total, pat, ngs, _) in enumerate(sug):\n",
    "                        for ng, _ in ngs:\n",
    "                            ngram = re.sub('\\([A-Z]+\\)', '', ng)\n",
    "                            if ans in ngram or ans2 in ngram:\n",
    "                                sub_score += float(len(sug)-num) / float(len(sug)+1e-10)\n",
    "                                hit = True\n",
    "                                break\n",
    "                        if hit:\n",
    "                            hit = False\n",
    "                            break\n",
    "            score[-1].append(float(sub_score)/(float(sub_total)+1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "idx = 0\n",
    "for k, v in evals.items():\n",
    "    if align[k] != 'X':\n",
    "        score_dict[align[k]] = score[idx]\n",
    "        idx+=1\n",
    "        \n",
    "with open('evaluation_score.json', 'w') as f:\n",
    "    json.dump(score_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 0.3166475321755383)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score), sum([sum(i) for i in score]) / sum([len(i) for i in score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
